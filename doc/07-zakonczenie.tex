\chapter{Podsumowanie i wnioski\label{chap:zakonczenie}}

Kilka dekad temu rozpoczął się proces informatyzacji przedsiębiorstw, który odkrył przed światową gospodarką nowe, do tej pory nieznane tory rozwoju. Zarządzania procesami w firmach i przedsiębiorstwach, czy też proces produkcyjny zostały znacząco skrócone dzięki wykorzystaniu skomputeryzowanych maszyn oraz komputerów w procesach kontroli. Do tej pory człowiek nie musiał sobie radzić z podobnymi wyzwaniami, które jednocześnie stanowiły dla człowieka możliwości do jeszcze lepszego wykorzystania ludzkiego potencjału. Należy podkreślić, że zmiany, jakie zaszły są praktycznie nieodwracalne. Zmusza to programistów do tworzenia nowych aplikacji, które będą w stanie sprostać wymaganiom stawianym przez użytkowników.

Jak napisano we wprowadzeniu do niniejszej pracy, informatzacja powinna realizować dwa podstawowe cele. Z~jednej strony powinna ona usprawniać pracę pojedynczego pracownika poprzez automatyzację realizowanych przez niego rutynowych zadań i obowiązków. Działania jednostki powinny być wykonywane szybciej i bardziej niezawodnie dzięki wykorzystaniu możliwości komputerów. Z drugiej strony informatyzacja powinna również wpływać na działania całych firm i insytucji poprzez wspomaganie procesu podejmowania decyzji przez kadrę zarządzającą przedsiębiorstwami (lub innymi jednostkami). Szybka analiza danych o stanie firmy może umożliwić podejmowanie trafniejszych decyzji przez kadrę zarządzającą - ma to duże znaczenie w przypadku decyzji o znaczeniu strategicznym dla rozwoju przedsiębiorstwa.

W chwili obecnej praktycznie w każdej przestrzeni ludzkiego życia znajduje się komputer, co wpłynęło na produkcję olbrzymich ilości danych. W większośći są one reprezentowane w sposób umożliwiający ich łatwe składowanie i przetwarzanie komputerowe przez aplikacje analityczne. Z roku na rok rozmiar danych produkowanych przez różnego rodzaju systemy komputerowe rośnie w oszałamiającym tempie. Dane te poddane analizie mogą przynieść wymierne korzyści zarówno w kwestiach finansowych, jak również poznawczych. Dzięki analizie zebranych w przeszłości informacji możliwe jest lepsze dopasowanie planów w przyszłości - na tej podstawie planowane mogą być np. akcje marketingowe, czy~też~promocje w supermarketach spożywczych. Wiedza uzyskana w ten sposób może być wykorzystana w bardzo różnorodny sposób, praktycznie w każdym obszarze działalności firmy.

Proces odkrywania zależności pomiędzy zgromadzonymi danymi bez wykorzystania systemów komputerowych jest niezwykle skomplikowany i wymaga do realizacji dużo czasu. W chwili obecnej analiza zgromadzonych danych przez człowieka jest praktycznie niemożliwa, bądź na tyle powolna, że wnioski nie będą przydatne, bo sytuacja ulegnie zmianie na tyle, że informacje z analizy przestaną być przydatne. Z tego też powodu tworzone są narzędzia umożliwiające odkrywanie prawidłowości w dużych zbiorach danych, by człowiek na tej podstawie mógł podejmować trafne i szybkie decyzje oraz wyciągać odpowiednie wnioski.

Eksploracja danych jest działem informatyki, który zajmuje się odkrywaniem ukrytych dla człowieka prawidłowości i reguł w danych. Jest to jeden z etapów procesu \termdef{odkrywania wiedzy z baz danych}. Eksploracja danych to proces odkrywania wiedzy w postaci nowych, użytecznych, poprawnych i zrozumiałych wzorców w bardzo dużych wolumenach danych~\cite{DataMiningStart}. Możliwości stosowania technik eksploracji danych w praktyce, wymagają efektywnych metod przeszukiwania ogromnych plików lub baz danych. Warto przy tym wspomnieć, że tego typu technologie nie są w chwili obecnej dobrze zintegrowane z systemami zarządzania bazami danych.

Eksploracja danych odbywa się najczęściej w środowisku baz lub hurtowni danych, które stanowią doskonałe źródła informacji do przeprowadzenia analizy - głównie ze względu na łatwość dostępu oraz usystematyzowaną strukturę przechowywanych informacji w postaci odpowiednich struktur danych. Bardzo często odkryte wzorce zapisuje się w osobnych relacjach bazy lub hurtowni danych, ponieważ liczba odkrytych wzorców w wielu przypadkach może być bardzo duża. Pozwala to na ich dalsze przetwarzanie w późniejszym czasie przez użytkowników końcowych. Pojęcie eksploracji zyskuje coraz większą popularność (również w wymiarze marketingowym) i jest wykorzystywane w wielu dziedzinach ludzkiego życia.

Reguły asocjacyjne są jednym z najczęściej wykorzystywanych modeli wiedzy w eksploracji danych. Jak przedstawiono w rozdziale~\ref{chap:teoria} reguła asocjacyjna ma postać $X \Rightarrow Y$, gdzie $X$ oraz $Y$ są wzajemnie rozłącznymi zbiorami elementów. Przykładem reguły, która mogła zostać odkryta w bazie danych sklepu komputerowego, może być reguła postaci $komputer \land myszka \Rightarrow monitor$. Prezentuje ona fakt, że klienci kupujący komputer oraz myszke z dużym prawdopodobieństwem kupią również monitor. Problem odkrywania reguł asocjacyjnych wraz z algorytmem Apriori, który jest podstawą wielu algorytmów znajdujących reguły asocjacyjne po raz pierwszy został sformułowany w~\cite{Problem:Statement}. Algorytm ten został następnie rozszerzony w pracy~\cite{AssRulesStrt} o bardziej uniwersalne sposoby generowania reguł. Innym rodzajem algorytmu odkrywającego reguły asocjacyjne jest FPGrowth, który opisany został w części~\ref{sec:fpgrowth} nieniejszej pracy.

W ostatnich latach pojawiły się nowe możliwości wykorzystania współczesnych komputerów. W roku 2007 firma NVIDIA udostępniła programistom uniwersalną architekturę obliczeniową CUDA (\english{Compute Unified Device Architecture}), który umożliwia wykorzystanie mocy obliczeniowej \termdef{procesorów graficznych} (\acronym{GPU}, \english{Graphics Processing Unit}), bądź innych procesorów wielordzeniowych, do rozwiązywania ogólnych problemów obliczeniowych w sposób znacząco wydajniejszy niż w przypadku tradycyjnych, sekwencyjnych procesorów~\cite{cuda:zone}. Choć w grach komputerowych moc obliczeniową jednostek graficznych można wykorzystać do obliczeń fizyki, to CUDA idzie jeszcze dalej, umożliwiając przyspieszenie obliczeń w takich dziedzinach, jak biologia, fizyka, kryptografia, bioinformatyka oraz innych naukach. Specjalnie dla potrzeb tego segmentu NVidia opracowała kartę graficzną o nazwie \emph{Tesla}~\cite{cuda:tesla}.

Do tej pory bardzo małe jest zainteresowanie wykorzystaniem tej~technologii w procesie odkrywania wiedzy, a~w~szczególności znajdowania reguł asocjacyjnych. Wyniki przeprowadzonych eksperymentów pozwalają przypuszczać, że algorytm wykorzystujący możliwości procesorów wielordzeniowych, a w szczególności GPU, będzie wyraźnie szybszy od klasycznych algorytmów eksploracji danych zaimplementowany na tradycyjnych procesorach. 

W niniejszej pracy opracowany został algorytm równoległy oparty na Apriori, który wykorzystuje możliwości wpółczesnych kart graficznych poprzez wykorzystanie technologii CUDA. Eksperymenty wykazały, że algorytm ten jest znacząco szybszy od innych implemntacji algorytmów z tej klasy, co pozwala na przetworzenie większych zbiorów danych w tych samych czasach.

\section{Dalszy rozwój}

W rozdziale~\ref{chap:eksperymenty} poświęconym eksperymentom zaprezentowano wyniki dla trzech algorytmów klasy apriori oraz klasycznego algorytmu FPGrowth. Wydajność tego drugiego znacząco przekraczała wydajności pozostałych algorytmów. W przyszłości celem jest przyspieszenie algorytmu FPGrowth poprzez zastosowanie technologii CUDA do zrównoleglenia obliczeń. Dodatkowo potrzebne byłoby przetestowanie algorytmów na większych zbiorach danych, gdzie odpowiedzi FPGrowth byłyby zależne od liczby transakcji.