
\chapter{Podstawy teoretyczne}

W rozdziale tym przedstawiony zostanie przegląd literatury, który stanowi podstawy wiedzy na temat eksploracji danych, a w szczególności problemu odkrywania reguł asocjacyjnych w dużych zbiorach danych. Zebrana wiedza posłużyła autorowi do opracowania algorytmu wykorzystującego możliwości współczesnych kart graficznych.

\section{Definicje}

\subsection{Model teoretyczny}

Niech $I = \lbrace i_1,i_2,...,i_m \rbrace$ będzie \emph{zbiorem elementów} o liczności $|I| = m$. \emph{Transakcją} nazwano dowolny, niepusty podzbiór $X \subseteq I$ zbioru elementów. Bazą danych $DB$ nazwano dowolny zbiór par $(id, X)$, gdzie $X$ jest transakcją, a $id$ jest dowolną wartością unikalną w ramach bazy danych nazywaną \emph{identyfikatorem transakcji}. Bez utraty ogólności założono iż $id\in \mathbb{N}$. 

\emph{Wsparciem} (\english{support}) $sup(X)$ transakcji $X$, w bazie danych nazwano częstość wystąpień transakcji w bazie danych. Formalnie przedstawia to wzór~\ref{support:def}.

\begin{equation}
\label{support:def}
sup(X)=\frac{| \lbrace id: (id,Y)\in DB \wedge X\subseteq Y \rbrace |}{|DB|}
\end{equation} 

Łatwo zauważyć, że jeśli poziom ten jest niski, to oznacza to, że nie ma jednoznacznych dowodów na łączne występowanie elementów zbioru $Z = X \cup Y$, ponieważ zbiór $Z$ występuje w niewielkiej liczbie transakcji. 

\begin{df}\label{regula:def}
Niech będą dane dwie transakcje $X$ i $Y$ takie, że $X\cap Y=\emptyset$. Implikację postaci $X\Rightarrow  Y$ nazwano \emph{regułą asocjacyjną}.
\end{df}

\emph{Poziom ufności} (\english{confidence}) jest miarą zdefiniowaną dla implikacji reprezentowanej przez regułę asocjacyjną~\cite{Elmasri:db}. 

\begin{df}\label{confidence:def}
Pioziom ufności ($conf$) reguły asocjacyjnej $X \Rightarrow Y$ jest równy 
\begin{equation}
	conf(X \Rightarrow Y) = \frac{sup(X \cup Y)}{sup(X)}
\end{equation}
\end{df}

Po analizie definicji~\ref{confidence:def} łatwo zauważyć, że poziom ufności może być interpretowany, jako estymacja prawdopodobieństwa $P(Y | X)$.


Reguły asocjacyjne zazwyczaj powinny spełniać pewne wymagania zdefiniowane przez użytkownika - minimalne wsparcie oraz minimalny poziom ufności, oznaczane odpowiednio $minsup$ oraz $minconf$. Wyznaczają one dla aplikacji progi, jakie powinny spełniać zbiory oraz reguły, aby były brane pod uwagę w trakcie analizy.

\begin{df}
Zbiorem częstym $X \subseteq I$ nazywamy taki zbiór, który spełnia zależność $sup(X) \geq minsup$.
\end{df}

Generowanie reguł asocjacyjnych zazwyczaj sprowadza się do dwóch, niezależnych kroków:
\begin{enumerate}
	\item Minimalne wsparcie jest używane do odnalezienia wszystkich zbiorów częstych w bazie danych $DB$.
	\item Znalezione zbiory często oraz minimalny poziom ufności są używane do wygenerownia reguł asocjacyjnych.
\end{enumerate}

Znalezienie wszystkich zbiorów częstych w bazie danych $DB$ jest zadaniem wymagającym przeszukania wszystkich możliwych kombinacji bez powtórzeń ze zbioru $I$. Zbiór możliwych zbiorów elementów ma liczność równą $2^n - 1$ (wszystkie zbiory, poza zbiorem pustym, który nie jest w tym wypadku zbiorem sensownym w znaczeniu poddania go analizie). 

Warto zauważyć, że dla każdego zbioru częstego $Y$, każdy jego podzbiór $X$ jest również zbiorem częstym~\cite{Problem:Statement}. Korzystając z tej właściwości wsparcia możliwe jest w sposób efektywny znalezienie wszystkich zbiorów częstych w zadanej bazie danych - z tej zależności korzysta algorytm apriori opisany w rozdziale~\ref{apriori:section}. Dodatkowo, wszystkie reguły zbudowane na podstawie zbioru częstego $Y$ muszą spełniać warunek minimalnego wsparcia, ponieważ spełnia ten warunek zbiór $Y$, a suma zbiorów reguły jest zbiorem wyjściowym $Y$.

\section{Aktualna wiedza}
W rozdziale tym zebrana została oraz opracowana dotychczasowa wiedza (\english{state-of-the-art}) na temat odkrywania reguł asocjacyjnych. Przedstawione zostaną dwa podstawowe algorytmy wykorzystywane w tym procesie: Apriori oraz FP-growth. W chwili obecnej te dwa algorytmy stanowią podstawę, na której budowane są nowe algorytmy, wykorzystujące możliwości współczesnych algorytmów (\emph{Czy wymieniać tutaj przykłady algorytmów, które bazują na nich + refy do kilku artykułów?}).

\subsection{Algorytm Apriori}\label{apriori:section}
Pierwszy algorytm odkrywający reguły asocjacyjne został przedstawiony w roku 1994 w pracy~\cite{Apriori:Main}. Niżej przedstawione zostaną szczegóły działania tego algorytmu nazwanego algorytmem \emph{Apriori}.

Zagadnienie odkrywania reguł asocjacyjnych można podzielić na dwa etapy~\cite{Problem:Statement}:
\begin{enumerate}
	\item Odkrywanie zbiorów częstych, których wartość wsparcia jest wyższa od wartości $minsup$.
	\item Generowanie reguł asocjacyjnych na podstawie znalezionych zbiorów częstych. Reguła $X \Rightarrow Y$ jest wynikiem działania algorytmu dla zbioru $Z = X \cup Y$, jeżeli spełnia ona nierówność $conf(X \Rightarrow Y) \geq minconf$. Ponieważ zbiór $Z = X \cup Y$ jest zbiorem częstym, to reguła spełnia również warunek przekraczania minimalnego wsparcia.

	Na tym etapie możliwe jest tworzenie reguł, w których w zbiorze \emph{poprzedników} ($X$ z oznaczeń z definicji~\ref{regula:def}) jest wiele elementów oraz jeden w \emph{następniku} (zbiór $Y$ z definicji~\ref{regula:def})~\cite{Problem:Statement} lub dopuszczana jest możliwość wielu elementów również w następniku~\cite{Apriori:Main}. W niniejszej pracy analizowany jest sposób generowania reguł, w którym oba zbiory mogą być zbiorami wieloelementowymi.
\end{enumerate}

W kolejnych podrozdziałach przedstawione zostaną etapy tworzące razem algorytmy Apriori.

\subsubsection{Generowanie zbiorów częstych}\label{apriori:gen}
W celu wyznaczenia zbiorów częstych algorytm dokonuje analizy bazy danych $DB$, by w kolejnych iteracjach generować rodziny coraz to liczniejszych zbiorów, będących zbiorami częstymi dla zadanej wartości $minsup$. Algorytm zaczyna od znalezienia wszystkich zbiorów jednoelementowych, które są zbiorami częstymi. W każdym kolejnym kroku generowane są zbiory częste na podstawie zbiorów wygenerowanych w kroku poprzednim. Proces ten jest kontynuowany do momentu aż nie zostaną znalezione żadne zbiory częste.

Algorytm generuje zbiory kandydatów jedynie na podstawie zbiorów częstych odkrytych w kroku poprzednim - co ważne generowanie ich odbywa się bez wielokrotnego przeglądania bazy danych transakcji. Intuicja podpowiada, że każdy podzbiór zbioru częstego jest zbiorem częstym. Zatem, każdy zbiór częsty zawierający $k$ elementów może być wygenerowany na podstawie połączenia dwóch zbiorów posiadających $k-1$ elementów, a na koniec kasując te zbiory, których jakikolwiek podzbiór nie jest częsty~\cite{Apriori:Main}.

\begin{table}
	\centering
	\begin{tabular}{|c|p{7.7cm}|} \hline
	$k$-zbiór & Zbiór zawierający $k$ elementów. \\ \hline
	$L_k$ & Zbiór zawierający $k$-zbiory. Każdy zbiór zawarty w $L_k$ zawiera dwa pola: i) zbiór oraz ii) wartość $support$. \\ \hline
	$C_k$ & Zbiór $k$-zbiorów kandydatów (potencjalnych zbiorów częstych). Każdy zbiór zawarty w $C_k$ zawiera dwa pola: i) zbiór oraz ii) wartość $support$. \\ \hline
	\end{tabular}\label{skroty:znaczanie}
	\caption{Oznaczenie skrótów w opisie algorytmu}
\end{table}

Tabela~\ref{skroty:znaczanie} zawiera spis oznaczeń używanych w opisie algorytmu. 

Procedura \proc{Apriori Frequent Set Generaion} przedstawia pseudokod realizujący opisywany w tym rozdziale algorytm generowania zbiorów częstych.

\begin{codebox}
	\Procname{$\proc{Apriori Frequent Set Generaion}$}\label{apriori:listing}
	\li $\id{L_1} \gets \lbrace 1$-zbiory częste $\rbrace$
		\li \For $(k = 2; \id{L_{k-1}} \neq \emptyset; k++)$
		\li \Do
			\li $\id{C_k} \gets aprioriGen(\id{L_{k-1}})$
			\li \textbf{forall} trasakcja $t \in \id{DB}$
			\li \Do
					\li $C_t \gets subset(C_k, t)$
					\li \textbf{forall} kandydat $c \in \id{C_t}$
					\li \Do c.count++
					\End
				\End
			\li $L_k \gets \lbrace c \in C_k | c.count \geq minsup \rbrace$	
		\End
	\li Answer $\gets \bigcup_k L_k $
\end{codebox}

\paragraph{Procedura aprioriGen}
Procedura \id{aprioriGen} reprezentuje proces twórzenia zbiorów $k$-elementowych kandydatów na podstawie zbiorów wejściowych ${k-1}$-elementowych. Procedura ta jest podzielona na dwa etapy: łączenia oraz przycinania. 

Jak łatwo zauważyć wynikiem działania \proc{Join Step} są zbiory $k$-elementowe, które powstały na podstawie zbiorów wejściowych $L_{k-1}$, a ich zawartość różni się tylko jednym elementem - ostatnim. Ważnym faktem jest to, iż elementy w zbiorach są uporządkowane leksykograficzne, co wykorzystywane jest w tej procedurze.

\begin{codebox}
	\Procname{$\proc{Join Step}$}
	\li \textbf{insert into} $C_k$
	\li \textbf{select} p.item$_1$, p.item$_2$, \dots, p.item$_{k-1}$, q.item$_{k-1}$
	\li \textbf{from} $L_{k-1}$ p, $L_{k-1}$ q
	\li \textbf{where} p.item$_1 = $ q.item$_1$, \dots, p.item$_{k-2}$ = q.item$_{k-2}$, p.item$_{k-1}$ $<$ q.item$_{k-1}$
\end{codebox}

Warto zauważyć, że \proc{Join Step} jest ekwiwalentem rozszerania zbioru $L_{k-1}$ każdym elementem zbioru elementów $I$, a następnie kasowania tych $(k-1)$-zbiorów otrzymanych przez usuwanie $(k-1)$ elementu, które nie są w $L_{k-1}$. 

Warunek p.item$_{k-1}$ $<$ q.item$_{k-1}$ zapewnia, że nie będą generowane duplikaty. Dlatego też po etapie łączenia zachodzi zależność $C_k \supseteq L_k$.

Następnym krokiem jest \proc{Prune Step}, w którym usuwane są wszystkie elementy $c \in C_k$, którego jakikolwiek podzbiór $(k-1)$-elementowy zbioru $c$ nie należy do $L_{k-1}$.

\begin{codebox}
	\Procname{$\proc{Prune Step}$}
		\li \textbf{forall} zbiór $c \in C_k$ 
		\li \Do
			\li \textbf{forall} $(k-1)$-podzbiór s zbioru $c$
					\li \Do 
						\li \If $s \notin L_{k-1}$
						\li \Then
							\li \textbf{delete} $c$ z $C_k$
						\End
					\End
		\End
\end{codebox}

Celem operacji przycinania (\english{prune}) jest ograniczenie rozmiaru zbioru $C_k$ przed sprawdzeniem wsparcia dla kandydatów w bazie danych $DB$. W tym celu wykorzystywana jest właściwość, z której wynika, że jeśli jakiś $(k-1)$-podzbiór danego kandydata ($c \in C_k$) nie występuje w $L_{k-1}$, to kandydatk $c$ nie jest zbiorem częstym i powinien być usunięty z $C_k$.

\subsubsection{Generowanie reguł asocjacyjnych}
Po zakończeniu pierwszego etapu algorytm przystępuje do drugiego, czyli do budowania reguł asocjacyjnych na podstawie odkrytych zbiorów. Podobnie, jak w~\cite{Apriori:Main} algorytm będący przedmiotem analizy niniejszej pracy, generuje wszystkie możliwe reguły asocjacyjne dla zadanego zbioru. Mniej ogólny sposób generowania reguł został przedstawiony w pracy~\cite{Problem:Statement}, jednakże podjęto decyzję, że jest to sposób zbyt mało użyteczny w środowisku produkcyjnym.

Aby wygenerować reguły, dla każdego zbioru częstego $l$ znajdowane są niepuste podzbiory - podzbiór taki oznaczony jest jako $a$. Dla takich oznaczeń wygenerowna zostanie reguła $a \Rightarrow (l-a)$, jeżeli spełniona jest nierówność $\frac{support(l)}{support(a)} \geq minconf$. Warto zauważyć, że dla każdego zbioru częstego generowane są wszystkie możliwe niepuste podzbiory - zapewnia to, że odkryte zostaną wszystkie możliwe reguły.

Procedura~\proc{Generate Frequent Itemsets} prezentuje generowanie reguł asocjacyjnych na podstawie odkrytych zbiorów częstych.

\begin{codebox}
	\Procname{$\proc{Generate Frequent Itemsets}$}
		\li \textbf{forall} zbiór częsty $l_k$, $k \geq 2$ \Do
			\li \textbf{call} genrules($l_k$, $l_k$)
					\End
		\End
\end{codebox}


\begin{codebox}
	\Procname{$\proc{Genrules}$($l_k$: $k$-zbiór częsty, $a_m$: $m$-zbiór częsty)}
		\li $\id{A} \gets \lbrace (m-1)$-zbiór $a_{m-1} | a_{m-1} \subset a_m \rbrace$
		\li \For $a_{m-1} \in A$
			\li \Do
			\li $\id{conf} \gets \frac{support(l_k)}{support(a_{m-1})}$
			\li \If $\id{conf} \geq \id{minconf}$
				\li \Then
						\li \textbf{output} reguła $a_{m-1} \Rightarrow (l_k - a_{m-1})$ \\ ufność = $conf$ oraz wsparcie= $support(l_k)$
						\li \If $m-1 > 1$ 
							\li \Then
							\li \textbf{call} genrules($l_k$, $a_{m-1}$) \\ generowanie reguł podzbiorów zbioru $a_{m-1}$
						\End
				\End
			\End
		\End
\end{codebox}

%Kluczową własnością wykorzystywaną w tym etapie jest antymonotoniczność funkcji wsparcia:

%Własność Apriori Niech X i Y będą zbiorami towarów. Jeśli X   Y, to 
%support(X)  support(Y)
%Dowód: X  Y  cover(X)  cover(Y)  support(X)  support(Y).

%Rysunek 3: Ilustracja własności Apriori: jeśli AB nie jest częsty, to możemy wykluczyć wszystkie jego nadzbiory
 
%Antymonotoniczność wsparcia ze względu na zawieranie się zbiorów oznacza, że rozszerzenie nieczęstego zbioru nie może prowadzić do częstego zbioru. Jeżeli zatem k-zbiór X nie jest częsty, wówczas możemy pominąć wszystkie zbiory Y, takie że X Í Y (por. rysunek 3).

%Niżej przedstawiamy szczegóły algorytmu, w którym własność Apriori jest wykorzystywana. Analizujemy, jak rodzina k-zbiorów częstych Lk powstaje z k-1 zbiorów częstych Lk-1.

%Dowolny k-zbiór nazywamy kandydatem, jeśli każdy jego (k-1)-podzbiór jest częsty. 

%Łączenie:
%Zakładamy, że towary w bazie transakcyjnej są ponumerowane i wszystkie zbiory są uporządkowane leksykograficznie w rosnącym porządku. Czyli każdy częsty l Î Lk-1 jest reprezentowany jako tablica 
%l = (l[1], l[2],...,l[k-1])
%gdzie l[1] < l[2] < ... < l[k-1]. Operacja łączenia Lk-1  Lk-1 jest wykonywana przez łączenie wszystkich par (k-1)-zbiorów częstych.

%Dwa zbiory l1,l2 Î Lk-1 zostają połączone, jeśli mają one k-2 takich samych elementów na początku, tzn. 
%(l1[1] = l2[1]) Ù (l1[2] = l2[2]) Ù...Ù(l1[k-2] = l2[k-2]) oraz (l1[k-1] < l2[k-1])
%(1)
%Warunek l1[k-1] < l2[k-1] jest po to, aby zapobiegać powstawaniu powtarzających się kandydatów. Wynikiem łączenia zbiorów l1 i l2 spełniających warunek jest k-elementowy zbiór l, który powstaje przez dołączenie l2[k-1] na końcu l1. 

%Przycinanie: Można zauważyć, że powstający zbiór kandydatów Ck jest nadzbiorem zbioru Lk, tzn. że jego elemnety mogą być częste lub nieczęste, ale wszystkie k-zbiory częste należą do Ck. Celem operacji przycinania jest redukowanie rozmiaru zbioru Ck kandydatów przed sprawdzaniem ich wsparcia w bazie transakcji. W tym celu wykorzystujemy własciwość Apriori, z której wynika, że jeśli jakiś (k-1)-podzbiór danego kandydata nie występuje w Lk-1, to ten kandydat powinien być usunięty z Ck. Algorytm sprawdzania obecności podzbiorów kandydatów w Lk-1 może być efektywnie implementowany za pomocą drzewa haszującego dla częstych zbiorów w Lk-1.

\subsection{Algorytm FP-growth}

Podstawową wadą algorytmu Apriori jest wysoki koszt przetważania dużych zbiorów danych. Przykładowo, dla $10^4$ 1-zbiorów częstych, algorytm Apriori wygeneruje około $10^7$ 2-zbiorów kandydatów, które następnie poddane zostaną weryfikacji, czy są zbiorami częstymi. Fakt ten stał się podstwawowym powodem, dla którego zaprojektowany oraz zaimplementowany został algorytm \emph{FP-growth} w pracy~\cite{Main:FPgrowth}. Algorytm ten pozwala wyeliminować konieczność generowania tak dużej liczby kandydujących zbiorów elementów. 

Algorytm FP-growth można podzielić na trzy podstawowe kroki.
\begin{enumerate}
	\item W kroku pierwszym generowana jest skompresowana wersja bazy danych $DB$, mająca postać drzewa częstych wzorców.W pierwszym dochodzi do kompresji bazy danych, 
	\item Drugim krokiem jest transformacja tak skonstruowanego drzewa do postaci FP-drzewa (patrz definicja~\ref{fptree:def}).
	\item Trzeci krok polega na analizie FP-drzewa celem odnalezienia reguł asocjacyjnych. W kroku tym stosowana jest metoda dziel i zwyciężaj (\english{divide-and-conquer}) zamiast podejścia Apriori. Takie podejście w sposób znaczący zmniejsza rozmiar 
\end{enumerate}

\subsubsection{Definicje}
\begin{df}\label{fptree:def}
FP-drzewo (\english{frequent-pattern tree}) jest to ukorzeniony, etykietowany w wierzchołkach graf acykliczny spełniający poniższe cechy.
\end{df}
\begin{enumerate}
	\item Korzeniem drzewa jest jeden element \id{null}, zbiór poddrzew prefixowanych elementami (jako dzieci elementu \id{null}) oraz tablicy wskaźników \id{element} $\rightarrow$ \emph{wskaźnik na element drzewa}.
	\item Każdy wierzchołek poddrzewa składa się z trzech elementów: nazwy element, licznika (\english{count}) oraz wskaźnika na inny wierzchołek. Nazwa elementu w sposób jednoznaczny identyfikuje element ze zbioru elementów $I$, licznik przechowuje liczbę transakcji reprezentowanych przez ścieżkę od \id{null} do tego elementu, natomiast wskaźnik wskazuje na kolejny wierzchołek w FP-drzewie, którego nazwa jest identyczna do danego.
	\item Każdy wpis w tablicy nagłówkowej (\english{frequent-item-header table}) skałada się z dwóch elementów: nazwy elementu oraz wskaźnika na pierwszy element w drzewie posiadający identyczną nazwę.
\end{enumerate}

\subsubsection{Konstrukcja FP-drzewa}
\textbf{Input:} Baza danych transakcji $DB$ oraz minimalne wsparcie (\id{minsupp}).

\textbf{Output:} FP-drzewo utworzone na podstawie zawartości $DB$

\textbf{Metoda:} Poniżej zostanie opisany proces konstrukcji FP-drzewa.

\begin{enumerate}
	\item Przeskanowanie bazy danych transakcji $DB$ odbywa się jednokrotnie. Utworzony na tej podstawie zostanie zbiór $F$, zawierający 1-zbiory częste. Posortowany malejąco zbiór $F$ na podstawie wartości $support$ dla każdego elementu tworzy listę $FList$, czyli listę wsystkich elementów tworzących jednoelementowe zbiory częste.
	\item f
\end{enumerate}

\subsubsection{Eksploracja FP-drzewa}

%Dzięki wykorzystaniu "downward-closure"~\cite{Problem:Statement} właściwości wsparcia, który gwarantuje, że wszystkie podzbiory zbioru częstego, również są zbiorami częstymi (i na odwrót), możliwe jest w sposób efektywny znalezienie wszystkich zbiorów częstych w zadaniej bazie danych.

%While the second step is straight forward, the first step needs more attention.
%Finding all frequent itemsets in a database is difficult since it involves searching all possible itemsets (item combinations). The set of possible itemsets is the power set over I and has size 2n - 1 (excluding the empty set which is not a valid itemset). Although the size of the powerset grows exponentially in the number of items n in I, efficient search is possible using the downward-closure property of support[2][4] (also called anti-monotonicity "Jian Pei, Jiawei Han, and Laks V.S. Lakshmanan. Mining frequent itemsets with convertible constraints. In Proceedings of the 17th International Conference on Data Engineering, April 2–6, 2001, Heidelberg, Germany, pages 433-442, 2001.") which guarantees that for a frequent itemset, all its subsets are also frequent and thus for an infrequent itemset, all its supersets must also be infrequent. Exploiting this property, efficient algorithms (e.g., Apriori[6] and Eclat[7]) can find all frequent itemsets.

%%%%%%%%%%%%%%%%%%%%%%%%%%% GARBAGE!!!

%Z uwagi na fakt, że należy w pewien sposób ograniczyć liczbę reguł, jako wyniku działania algorytmu, wprowadza się dwie wartości $minsup$ oraz $minconf$. Wszystkie kombinacje przedmiotów ($\mathbf{X}_k$), które spełniają nierówność $sup(\mathbf{X}_k) \leq minsup$ nazywane są zbiorami dużymi (LARGE?). Pozostałe zbiory nazywane są zbiorami małymi (SMALL?)~\cite{Problem:Statement}.

%Ze wzoru wynika, że poziom ufności jest estymacją prawdopodobieństwa tego, że elementy tworzące zbiór $\mathbf{Y}$ zostaną kupione przez danego klienta pod warunkiem, że klient kupi elementy należące do zbioru $\mathbf{X}$.

%Inną motywacją, poza statystyczną ważnością, dlaczego interesujący jest poziom pokrycia, jest fakt, że poszukiwane reguły powinny spełniać pewne wymaganie, co do wysokości wartości $sup$ z powodów biznesowych. Jeśli poziom pokrycia nie jest wystarczająco wysoki, to oznacza to, iż reguła nie jest warta brania pod uwagę, bądź jest ona mniej preferowana (może być rozpatrzona później)~\cite{Problem:Statement}. Dlatego też w przypadku reguły asocjacyjnej określenie wartości $sup$ dla każdej reguły i weryfikowanie tylko tych, które spełniają pewne wymagania co do wysokości tego współczynnika.



%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Reguła asocjacyjna zdefiniowana w Definicji~\ref{regula:def} określa, że jeśli dany klient kupił towary ze zbioru $\mathbf{X}$, najprawdopodobniej kupi też towary ze zbioru $\mathbf{Y}$. Aby reguła asocjacyjna stanowiła interesujące źródło informacji dla analityka stosującego techniki eksploracji danych, musi ona spełniać określone warunki wyrażone za pomocą odpowiednich miar. Dwie najbardziej popularne miary jakości reguł asocjacyjnych to poziom pokrycia (\\english{support}) oraz poziom ufności (\english{confidence}).


%Oznacza to, że poziom pokrycia ($sup$) jest stosunkiem liczby transakcji zawierających elementy sumy zbiorów (czyli $|\mathbf{Z}|$, gdzie $\mathbf{Z} = \mathbf{X} \cup \mathbf{Y}$) do liczby wszystkich transakcji w systemie ($|\mathbf{T}|$). Wzór \ref{support2:def} przedstawia sposó obliczania wartości $sup$.

%\begin{equation}\label{support2:def}
	%sup = \frac{|\mathbf{Z}|}{|\mathbf{T}|}
%\end{equation}